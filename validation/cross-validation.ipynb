{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53d7633-fc64-465a-8a60-7c51a1bc5cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "from tqdm import tqdm\n",
    "from __future__ import print_function\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db447cb0-7b65-432d-8bea-cbb48ec492e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f88b077-cff0-4fe6-9239-9cf512dcdcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples(filepath):\n",
    "    \"\"\"Yields examples.\"\"\"\n",
    "    # Yields (key, example) tuples from the dataset\n",
    "    with open(filepath, encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        for example in data[\"data\"]:\n",
    "            title = example.get(\"title\", \"\").strip()\n",
    "            for paragraph in example[\"paragraphs\"]:\n",
    "                context = paragraph[\"context\"].strip()\n",
    "                for qa in paragraph[\"qas\"]:\n",
    "                    question = qa[\"question\"].strip()\n",
    "                    id_ = str(qa[\"id\"])\n",
    "\n",
    "                    answer_starts = [answer[\"answer_start\"] for answer in qa[\"answers\"]]\n",
    "                    answers = [answer[\"text\"].strip() for answer in qa[\"answers\"]]\n",
    "\n",
    "                    yield id_, {\n",
    "                        \"title\": title,\n",
    "                        \"context\": context,\n",
    "                        \"question\": question,\n",
    "                        \"id\": id_,\n",
    "                        \"answers\": {\n",
    "                            \"answer_start\": answer_starts,\n",
    "                            \"text\": answers,\n",
    "                        },\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889371ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"./output-eval-strategy/checkpoint-20000/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./output-eval-strategy/checkpoint-20000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19538cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/xquad/xquad.tr.json\", \"rb\") as f:\n",
    "    xquad = json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0476c2-2d2f-4604-b606-9750f557b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\n",
    "    'question-answering', \n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f8d2dd-70d8-4027-8ad0-fdff9c4adfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = exact_match = total = 0\n",
    "for article in tqdm(xquad[\"data\"]):\n",
    "    for paragraph in article['paragraphs']:\n",
    "        for qa in paragraph['qas']:\n",
    "            total += 1\n",
    "\n",
    "            ground_truths = list(map(lambda x: x['text'], qa['answers']))\n",
    "            prediction = nlp({'question': qa[\"question\"], 'context':  paragraph[\"context\"]})[\"answer\"]\n",
    "    \n",
    "            exact_match += metric_max_over_ground_truths(\n",
    "                exact_match_score, prediction, ground_truths)\n",
    "            f1 += metric_max_over_ground_truths(\n",
    "                f1_score, prediction, ground_truths)\n",
    "\n",
    "exact_match = 100.0 * exact_match / total\n",
    "f1 = 100.0 * f1 / total\n",
    "\n",
    "result = {'exact_match': exact_match, 'f1': f1}\n",
    "print(\"Results on XQuAD\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4803363-f268-4636-9b53-193f702f4579",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = exact_match = total = 0\n",
    "\n",
    "for subfolder in os.scandir(\"./data/thquad\"):\n",
    "    for data_file in os.scandir(subfolder):\n",
    "        examples = generate_examples(data_file.path)\n",
    "        for example in tqdm(examples):\n",
    "            total += 1\n",
    "\n",
    "            ground_truths = example[1][\"answers\"][\"text\"]\n",
    "            prediction = nlp(\n",
    "                {\n",
    "                    'question': example[1][\"question\"],\n",
    "                    'context':  example[1][\"context\"]\n",
    "                }\n",
    "            )\n",
    "            prediction = prediction[\"answer\"]\n",
    "            exact_match += metric_max_over_ground_truths(\n",
    "                exact_match_score, prediction, ground_truths)\n",
    "            f1 += metric_max_over_ground_truths(\n",
    "                f1_score, prediction, ground_truths)\n",
    "\n",
    "exact_match = 100.0 * exact_match / total\n",
    "f1 = 100.0 * f1 / total\n",
    "\n",
    "result = {'exact_match': exact_match, 'f1': f1}\n",
    "print(\"Results on THQuAD\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
